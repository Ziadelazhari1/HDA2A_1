# HDA2A: The World's First Ultra-Reasoning Framework

> **HDA2A** (Hierarchical Distributed Agent-to-Agent) is a simple yet powerful reasoning framework that enhances the capabilities of existing Large Language Models (LLMs) using *only* prompt engineering—no fine-tuning, no retraining, no special tools.

## 🧠 What is HDA2A?

HDA2A introduces a novel architecture where multiple specialized LLM agents interact hierarchically and cooperatively. This agent-to-agent prompting system enables the model to reason more deeply, self-correct, and maintain logical coherence across complex tasks.

It is designed to:

* 🚫 Minimize hallucinations
* 🧠 Maximize multi-step reasoning
* ⚡ Work plug-and-play with any chat-based LLM (e.g., ChatGPT, Claude, DeepSeek)
* 🔧 Require *no fine-tuning* or API-level modifications—just smart prompts

## 📄 Whitepaper

Read the full technical whitepaper in the repository or on zenodo to understand the architecture, experiments, and applications.

## 🧪 Tested Use Cases

* Solving Olympiad-level math problems (IMO)
* Investigating unsolved conjectures (e.g., Erdős–Straus)
* Technical research breakdowns (Graphene, AGI)
* Abstract reasoning tasks (ARC)

## 📌 Key Features

* Modular agent design
* Chain-of-thought reflection and critique
* Role-based reasoning orchestration
* Surprising emergent self-correction

## 🚀 Getting Started

While an automatic version of HDA2A hasn't been made due to budget constraints, we hope that this framework is known enough in the AI community that we can proceed to code better versions.
In the meantime, you are encouraged to try it manually using the prompts in the repository

## 🤝 Contributing

This is a living framework. If you experiment with HDA2A and find new use cases, roles, or improvements, feel free to open an issue or PR.

## 📢 License

MIT License — free to use, modify, and share.
